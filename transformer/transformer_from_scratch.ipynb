{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Transformer Architecture: A Step-by-Step Guide\n",
    "\n",
    "## Paper Reference\n",
    "- [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017)\n",
    "- Key sections: \n",
    "  - 3.1: Encoder and Decoder Stacks\n",
    "  - 3.2: Attention Mechanism\n",
    "  - 3.3: Position-wise Feed-Forward Networks\n",
    "  - 3.4: Embeddings and Softmax\n",
    "  - 3.5: Positional Encoding\n",
    "  - 5.4: Regularization (dropout strategy)\n",
    "\n",
    "## Implementation Strategy\n",
    "Breaking down the architecture into manageable pieces and gradually adding complexity:\n",
    "\n",
    "1. Start with foundational components:\n",
    "   - Embedding + Positional Encoding\n",
    "   - Single-head self-attention\n",
    "   \n",
    "2. Build up attention mechanism:\n",
    "   - Extend to multi-head attention\n",
    "   - Add cross-attention capability\n",
    "   - Implement attention masking\n",
    "\n",
    "3. Construct larger components:\n",
    "   - Encoder (self-attention + FFN)\n",
    "   - Decoder (masked self-attention + cross-attention + FFN)\n",
    "   \n",
    "4. Combine into final architecture:\n",
    "   - Encoder-Decoder stack\n",
    "   - Full Transformer with input/output layers\n",
    "\n",
    "## Development Tips\n",
    "1. Visualization and Planning:\n",
    "   - Draw out tensor dimensions on paper\n",
    "   - Sketch attention patterns and masks\n",
    "   - Map each component back to paper equations\n",
    "   - This helps catch dimension mismatches early!\n",
    "\n",
    "2. Dimension Cheat Sheet:\n",
    "   - Input tokens: [batch_size, seq_len]\n",
    "   - Embeddings: [batch_size, seq_len, d_model]\n",
    "   - Attention matrices: [batch_size, num_heads, seq_len, seq_len]\n",
    "   - FFN hidden layer: [batch_size, seq_len, d_ff]\n",
    "   - Output logits: [batch_size, seq_len, vocab_size]\n",
    "\n",
    "3. Common Pitfalls:\n",
    "   - Forgetting to scale dot products by √d_k\n",
    "   - Applying mask too early or too late\n",
    "   - Incorrect mask dimensions or application\n",
    "   - Missing residual connections\n",
    "   - Wrong order of layer norm and dropout\n",
    "   - Tensor dimension mismatches in attention\n",
    "   - Not handling padding properly\n",
    "\n",
    "4. Performance Considerations:\n",
    "   - Memory usage scales with sequence length squared\n",
    "   - Attention computation is O(n²) with sequence length\n",
    "   - Balance between d_model and num_heads\n",
    "   - Trade-off between model size and batch size\n",
    "\n",
    "## Testing Strategy\n",
    "- Test each component independently\n",
    "- Verify shape preservation\n",
    "- Check attention patterns\n",
    "- Confirm mask effectiveness\n",
    "- Validate gradient flow\n",
    "- Monitor numerical stability\n",
    "\n",
    "Remember: The key to successfully implementing the Transformer is understanding how each piece fits together and maintaining clear dimension tracking throughout the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: https://huggingface.co/datasets/bird-of-paradise/transformer-from-scratch-tutorial/blob/main/Transformer_Implementation_Tutorial.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Section\n",
    "### Embedding and Positional Encoding\n",
    "This implements the input embedding from Section 3.4 and positional encoding from Section 3.5 of the paper. Key points:\n",
    "- Embedding dimension can differ from model dimension (using projection)\n",
    "- Positional encoding uses sine and cosine functions\n",
    "- Scale embeddings by $\\sqrt{d_{model}}$\n",
    "- Apply dropout to the sum of embeddings and positional encodings\n",
    "\n",
    "Implementation tips:\n",
    "- Use `nn.Embedding` for token embeddings\n",
    "- Store scaling factor as float during initialization\n",
    "- Remember to expand positional encoding for batch dimension\n",
    "- Add assertion for input dtype (should be torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# 输入和输出词表的 embedding 都可以使用这个类\n",
    "class EmbeddingWithProjection(nn.Module):\n",
    "    def __init__(self, vocab_size, d_embed, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_embed = d_embed\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.d_embed)\n",
    "        self.embd_proj = nn.Linear(self.d_embed, self.d_model)\n",
    "        self.scaling = float(math.sqrt(self.d_model))\n",
    "        self.layernorm = nn.LayerNorm(self.d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_positional_encoding(seq_length, d_model, batch_size=1):\n",
    "        # Create position indices: [seq_length, 1]\n",
    "        position = torch.arange(seq_length).unsqueeze(1).float()\n",
    "\n",
    "        # Create dimension indices: [1, d_model//2]\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        # Create empty tensor: [seq_length, d_model]\n",
    "        pe = torch.zeros(seq_length, d_model)\n",
    "\n",
    "        # Compute sin and cos\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add batch dimension and expand: [batch_size, seq_length, d_model]\n",
    "        pe = pe.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 在进行embedding查找之前需要确定输入满足下标的要求，所以输入类型必须是torch.long\n",
    "        assert (\n",
    "            x.dtype == torch.long\n",
    "        ), f\"Input tensor must have dtype torch.long, got {x.dtype}\"\n",
    "        batch_size, seq_length = x.size()  # [batch, seq_length]\n",
    "\n",
    "        # token embedding\n",
    "        token_embedding = self.embedding(x)  # [batch_size, seq_length, d_embed]\n",
    "        # project the scaled token embedding to the d_model space\n",
    "        token_embedding = (\n",
    "            self.embd_proj(token_embedding) * self.scaling\n",
    "        )  # [batch_size, seq_length, d_model]\n",
    "\n",
    "        # add positional encodings to projected,\n",
    "        # scaled embeddings before applying layer norm and dropout.\n",
    "        positional_encoding = self.create_positional_encoding(\n",
    "            seq_length, self.d_model, batch_size\n",
    "        )  # [batch_size, seq_length, d_model]\n",
    "\n",
    "        # In addition, we apply dropout to the sums of the embeddings\n",
    "        # in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\n",
    "        normalized_sum = self.layernorm(token_embedding + positional_encoding)\n",
    "        final_output = self.dropout(normalized_sum)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 模型结构 ===\n",
      "EmbeddingWithProjection(\n",
      "  (embedding): Embedding(50000, 1024)\n",
      "  (embd_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "=== 参数统计 ===\n",
      "总参数量: 51,725,824 (51.73M)\n",
      "可训练参数: 51,725,824 (51.73M)\n",
      "不可训练参数: 0 (0)\n",
      "\n",
      "=== 每层参数详情 ===\n",
      "Layer Name                     Parameters      Shape\n",
      "-----------------------------------------------------------------\n",
      "embedding.weight               51.20M          [50000, 1024]\n",
      "embd_proj.weight               524.29K         [512, 1024]\n",
      "embd_proj.bias                 512             [512]\n",
      "layernorm.weight               512             [512]\n",
      "layernorm.bias                 512             [512]\n",
      "-----------------------------------------------------------------\n",
      "Total                          51.73M         \n",
      "torch.Size([1, 20])\n",
      "tensor([[ 6370,  5742, 33006, 38192, 12627, 41345, 25454, 45323,  8666, 43669,\n",
      "         11891, 48564,  6534, 17972, 31193, 29944, 16869, 25063, 19581, 10975]])\n",
      "torch.Size([1, 20, 512])\n",
      "tensor([[[-1.3060, -2.2055, -1.4481,  ..., -0.1463, -0.3235, -0.8731],\n",
      "         [ 0.3559,  2.7518,  0.0000,  ...,  0.4895, -0.2296, -0.7324],\n",
      "         [-0.4690, -0.2929,  1.6223,  ..., -0.8573,  0.1003,  0.7132],\n",
      "         ...,\n",
      "         [ 0.2033,  0.1347,  0.4135,  ...,  2.4910, -0.2189, -1.4437],\n",
      "         [ 0.7869,  1.3667,  1.2908,  ...,  0.0000, -0.2945, -0.0000],\n",
      "         [-1.5102,  0.2182, -1.1659,  ...,  2.7092,  0.0043, -1.1671]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 方法1: 总参数量\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "# 方法2: 区分可训练和不可训练参数\n",
    "def count_parameters_detailed(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "    total = trainable + non_trainable\n",
    "    return {\"trainable\": trainable, \"non_trainable\": non_trainable, \"total\": total}\n",
    "\n",
    "\n",
    "# 方法3: 格式化显示（以百万为单位）\n",
    "def format_parameter_count(count):\n",
    "    if count >= 1e9:\n",
    "        return f\"{count/1e9:.2f}B\"\n",
    "    elif count >= 1e6:\n",
    "        return f\"{count/1e6:.2f}M\"\n",
    "    elif count >= 1e3:\n",
    "        return f\"{count/1e3:.2f}K\"\n",
    "    else:\n",
    "        return str(count)\n",
    "\n",
    "\n",
    "# 方法4: 显示每层参数量\n",
    "def print_model_structure(model):\n",
    "    total_params = 0\n",
    "    print(f\"{'Layer Name':<30} {'Parameters':<15} {'Shape'}\")\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        param_count = param.numel()\n",
    "        total_params += param_count\n",
    "        print(\n",
    "            f\"{name:<30} {format_parameter_count(param_count):<15} {str(list(param.shape))}\"\n",
    "        )\n",
    "\n",
    "    print(\"-\" * 65)\n",
    "    print(f\"{'Total':<30} {format_parameter_count(total_params):<15}\")\n",
    "    return total_params\n",
    "\n",
    "\n",
    "# 测试 EmbeddingWithProjection 模型\n",
    "vocab_size = 50000\n",
    "d_embed = 1024\n",
    "d_model = 512\n",
    "\n",
    "embd = EmbeddingWithProjection(vocab_size, d_embed, d_model)\n",
    "\n",
    "\n",
    "def custom_show_model_detail(model):\n",
    "    print(\"=== 模型结构 ===\")\n",
    "    print(model)\n",
    "    print(\"\\n=== 参数统计 ===\")\n",
    "\n",
    "    # 简单统计\n",
    "    total_params = count_parameters(model)\n",
    "    print(f\"总参数量: {total_params:,} ({format_parameter_count(total_params)})\")\n",
    "\n",
    "    # 详细统计\n",
    "    detailed = count_parameters_detailed(model)\n",
    "    print(\n",
    "        f\"可训练参数: {detailed['trainable']:,} ({format_parameter_count(detailed['trainable'])})\"\n",
    "    )\n",
    "    print(\n",
    "        f\"不可训练参数: {detailed['non_trainable']:,} ({format_parameter_count(detailed['non_trainable'])})\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== 每层参数详情 ===\")\n",
    "    print_model_structure(model)\n",
    "\n",
    "\n",
    "custom_show_model_detail(embd)\n",
    "\n",
    "x = torch.randint(0, vocab_size, (1, 20))\n",
    "print(x.shape)\n",
    "print(x)\n",
    "output = embd(x)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Attention\n",
    "Implements the core attention mechanism from Section 3.2.1. Formula: Attention(Q,K,V) = softmax(QK^T/ $\\sqrt{d_k}$ )V\n",
    "\n",
    "Key points:\n",
    "- Supports both self-attention and cross-attention\n",
    "- Multi-head attention implementation per section 3.2.2\n",
    "- Handles different sequence lengths for encoder/decoder\n",
    "- Scales dot products by 1/√d_k\n",
    "- Applies attention masking before softmax\n",
    "- Applies dropout after softmax\n",
    "\n",
    "Implementation tips:\n",
    "- Use separate Q,K,V projections\n",
    "- Handle masking through addition (not masked_fill)\n",
    "- Remember to use braodcasting and reshape for multi-head attention\n",
    "- Keep track of tensor dimensions at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "class TransformerAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Scaled Dot Product Attention Module\n",
    "    Args:\n",
    "        d_model: Total dimension of the model.\n",
    "        num_head: Number of attention heads.\n",
    "        dropout: Dropout rate for attention scores.\n",
    "        bias: Whether to include bias in linear projections.\n",
    "\n",
    "    Inputs:\n",
    "        sequence: input sequence for self-attention and the query for cross-attention\n",
    "        key_value_state: input for the key, values for cross-attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, d_model, num_head, dropout=0.1, bias=True\n",
    "    ):  # infer d_k, d_v, d_q from d_model\n",
    "        super().__init__()  # Missing in the original implementation\n",
    "        assert d_model % num_head == 0, \"d_model must be divisible by num_head\"\n",
    "        self.d_model = d_model\n",
    "        self.num_head = num_head\n",
    "        self.d_head = d_model // num_head\n",
    "        self.dropout_rate = dropout  # Store dropout rate separately\n",
    "\n",
    "        # linear transformations\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.output_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Initiialize scaler\n",
    "        self.scaler = float(\n",
    "            1.0 / math.sqrt(self.d_head)\n",
    "        )  # Store as float in initialization\n",
    "\n",
    "    def forward(self, x, encoder_output=None, att_mask=None):\n",
    "        \"\"\"Input shape: [batch_size, seq_len, d_model=num_head * d_head]\"\"\"\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "\n",
    "        # Check only critical input dimensions\n",
    "        assert (\n",
    "            d_model == self.d_model\n",
    "        ), f\"Input dimension {d_model} doesn't match model dimension {self.d_model}\"\n",
    "        if encoder_output is not None:\n",
    "            assert (\n",
    "                encoder_output.size(-1) == self.d_model\n",
    "            ), f\"Cross attention key/value dimension {encoder_output.size(-1)} doesn't match model dimension {self.d_model}\"\n",
    "\n",
    "        # if encoder_output are provided this layer is used as a cross-attention layer\n",
    "        # for the decoder\n",
    "        is_cross_attention = encoder_output is not None\n",
    "\n",
    "        # Linear projections and reshape for multi-head\n",
    "        Q_state = self.q_proj(x)\n",
    "        if is_cross_attention:\n",
    "            kv_seq_len = encoder_output.size(1)\n",
    "            K_state = self.k_proj(encoder_output)\n",
    "            V_state = self.v_proj(encoder_output)\n",
    "        else:\n",
    "            kv_seq_len = seq_len\n",
    "            K_state = self.k_proj(x)\n",
    "            V_state = self.v_proj(x)\n",
    "\n",
    "        # [batch_size, self.num_head, seq_len, self.d_head]\n",
    "        Q_state = Q_state.view(\n",
    "            batch_size, seq_len, self.num_head, self.d_head\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        # in cross-attention, key/value sequence length might be different from query sequence length\n",
    "        K_state = K_state.view(\n",
    "            batch_size, kv_seq_len, self.num_head, self.d_head\n",
    "        ).transpose(1, 2)\n",
    "        V_state = V_state.view(\n",
    "            batch_size, kv_seq_len, self.num_head, self.d_head\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        # Compute attention matrix: QK^T, result shape: [batch_size, num_head, seq_len, kv_seq_len]\n",
    "        self.att_matrix = torch.matmul(Q_state, K_state.transpose(-1, -2)) * self.scaler\n",
    "\n",
    "        # apply attention mask to attention matrix\n",
    "        if att_mask is not None and not isinstance(att_mask, torch.Tensor):\n",
    "            raise TypeError(\"att_mask must be a torch.Tensor\")\n",
    "\n",
    "        if att_mask is not None:\n",
    "            self.att_matrix = self.att_matrix + att_mask\n",
    "\n",
    "        # apply softmax to the last dimension to get the attention score: softmax(QK^T)\n",
    "        # result shape: [batch_size, num_head, seq_len, kv_seq_len]\n",
    "        att_score = F.softmax(self.att_matrix, dim=-1)\n",
    "\n",
    "        # apply drop out to attention score, result shape: [batch_size, num_head, seq_len, kv_seq_len]\n",
    "        att_score = self.dropout(att_score)\n",
    "\n",
    "        # get final output: softmax(QK^T)V, result shape: [batch_size, num_head, seq_len, d_head]\n",
    "        att_output = torch.matmul(att_score, V_state)\n",
    "\n",
    "        # concatinate all attention heads\n",
    "        att_output = att_output.transpose(\n",
    "            1, 2\n",
    "        )  # [batch_size, seq_len, num_head, d_head]\n",
    "        att_output = att_output.contiguous().view(\n",
    "            batch_size, seq_len, self.num_head * self.d_head\n",
    "        )  # [batch_size, seq_len, d_model]\n",
    "\n",
    "        # final linear transformation to the concatenated output\n",
    "        att_output = self.output_proj(att_output)  # [batch_size, seq_len, d_model]\n",
    "\n",
    "        assert att_output.size() == (\n",
    "            batch_size,\n",
    "            seq_len,\n",
    "            self.d_model,\n",
    "        ), f\"Final output shape {att_output.size()} incorrect\"\n",
    "\n",
    "        return att_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 模型结构 ===\n",
      "TransformerAttention(\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (output_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "=== 参数统计 ===\n",
      "总参数量: 1,050,624 (1.05M)\n",
      "可训练参数: 1,050,624 (1.05M)\n",
      "不可训练参数: 0 (0)\n",
      "\n",
      "=== 每层参数详情 ===\n",
      "Layer Name                     Parameters      Shape\n",
      "-----------------------------------------------------------------\n",
      "q_proj.weight                  262.14K         [512, 512]\n",
      "q_proj.bias                    512             [512]\n",
      "k_proj.weight                  262.14K         [512, 512]\n",
      "k_proj.bias                    512             [512]\n",
      "v_proj.weight                  262.14K         [512, 512]\n",
      "v_proj.bias                    512             [512]\n",
      "output_proj.weight             262.14K         [512, 512]\n",
      "output_proj.bias               512             [512]\n",
      "-----------------------------------------------------------------\n",
      "Total                          1.05M          \n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "d_head = 8\n",
    "attn = TransformerAttention(d_model, d_head, dropout=0.1, bias=True)\n",
    "custom_show_model_detail(attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-Forward Network (FFN)\n",
    "Implements the position-wise feed-forward network from Section 3.3: FFN(x) = max(0, xW₁ + b₁)W₂ + b₂\n",
    "\n",
    "Key points:\n",
    "- Two linear transformations with ReLU in between\n",
    "- Inner layer dimension (d_ff) is typically 2048\n",
    "- Applied identically to each position\n",
    "\n",
    "Implementation tips:\n",
    "- Use nn.Linear for transformations\n",
    "- Remember to include bias terms\n",
    "- Position-wise means same transformation for each position\n",
    "- Dimension flow: d_model → d_ff → d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Networks\n",
    "    This consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "    FFN(x) = max(0, xW1 + b1 )W2 + b2\n",
    "    d_model: embedding dimension (e.g., 512)\n",
    "    d_ff: feed-forward dimension (e.g., 2048)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        # Linear transformation y = xW+b\n",
    "        self.fc1 = nn.Linear(self.d_model, self.d_ff, bias=True)\n",
    "        self.fc2 = nn.Linear(self.d_ff, self.d_model, bias=True)\n",
    "\n",
    "        # for potential speed up\n",
    "        # Pre-normalize the weights (can help with training stability)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # check input and first FF layer dimension matching\n",
    "        batch_size, seq_length, d_input = input.size()\n",
    "        assert (\n",
    "            self.d_model == d_input\n",
    "        ), \"d_model must be the same dimension as the input\"\n",
    "\n",
    "        # First linear transformation followed by ReLU\n",
    "        # There's no need for explicit torch.max() as F.relu() already implements max(0,x)\n",
    "        f1 = F.relu(self.fc1(input))\n",
    "\n",
    "        # max(0, xW_1 + b_1)W_2 + b_2\n",
    "        f2 = self.fc2(f1)\n",
    "\n",
    "        return f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN(\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = FFN(d_model=512, d_ff=2048)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder\n",
    "Implements single encoder layer from Section 3.1, consisting of:\n",
    "- Multi-head self-attention\n",
    "- Position-wise feed-forward network\n",
    "- Residual connections and layer normalization\n",
    "\n",
    "\n",
    "Implementation tips:\n",
    "- Apply dropout before adding residual\n",
    "- Keep model dimension consistent through the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder layer of the Transformer\n",
    "    Sublayers: TransformerAttention\n",
    "               Residual LayerNorm\n",
    "               FNN\n",
    "               Residual LayerNorm\n",
    "    Args:\n",
    "            d_model: 512 model hidden dimension\n",
    "            d_embed: 512 embedding dimension, same as d_model in transformer framework\n",
    "            d_ff: 2048 hidden dimension of the feed forward network\n",
    "            num_head: 8 Number of attention heads.\n",
    "            dropout:  0.1 dropout rate\n",
    "\n",
    "            bias: Whether to include bias in linear projections.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, num_head, dropout=0.1, bias=True):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        # attention sublayer\n",
    "        self.att = TransformerAttention(\n",
    "            d_model=d_model, num_head=num_head, dropout=dropout, bias=bias\n",
    "        )\n",
    "\n",
    "        # FFN sublayer\n",
    "        self.ffn = FFN(d_model=d_model, d_ff=d_ff)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # layer-normalization layer\n",
    "        self.LayerNorm_att = nn.LayerNorm(self.d_model)\n",
    "        self.LayerNorm_ffn = nn.LayerNorm(self.d_model)\n",
    "\n",
    "    def forward(self, embed_input, padding_mask=None):\n",
    "\n",
    "        batch_size, seq_len, _ = embed_input.size()\n",
    "\n",
    "        ## First sublayer: self attion\n",
    "        att_sublayer = self.att(\n",
    "            sequence=embed_input, key_value_states=None, att_mask=padding_mask\n",
    "        )  # [batch_size, sequence_length, d_model]\n",
    "\n",
    "        # apply dropout before layer normalization for each sublayer\n",
    "        att_sublayer = self.dropout(att_sublayer)\n",
    "        # Residual layer normalization\n",
    "        att_normalized = self.LayerNorm_att(\n",
    "            embed_input + att_sublayer\n",
    "        )  # [batch_size, sequence_length, d_model]\n",
    "\n",
    "        ## Second sublayer: FFN\n",
    "        ffn_sublayer = self.ffn(\n",
    "            att_normalized\n",
    "        )  # [batch_size, sequence_length, d_model]\n",
    "        ffn_sublayer = self.dropout(ffn_sublayer)\n",
    "        ffn_normalized = self.LayerNorm_ffn(\n",
    "            att_normalized + ffn_sublayer\n",
    "        )  # [batch_size, sequence_length, d_model]\n",
    "\n",
    "        return ffn_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerEncoder(\n",
      "  (att): TransformerAttention(\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (output_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ffn): FFN(\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (LayerNorm_att): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (LayerNorm_ffn): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = TransformerEncoder(d_model=512, d_ff=2048, num_head=8, dropout=0.1, bias=True)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Decoder\n",
    "Implements decoder layer from Section 3.1, with three sub-layers:\n",
    "- Masked multi-head self-attention\n",
    "- Multi-head cross-attention with encoder output\n",
    "- Position-wise feed-forward network\n",
    "\n",
    "Key points:\n",
    "- Self-attention uses causal masking\n",
    "- Cross-attention allows attending to all encoder outputs\n",
    "- Each sub-layer followed by residual connection and layer normalization\n",
    "- Apply dropout to the output of previous sub-layer before residual connection and layer normalization\n",
    "\n",
    "Implementation tips:\n",
    "- Order of operations matters (masking before softmax)\n",
    "- Each attention layer has its own projections\n",
    "- Remember to pass encoder outputs for cross-attention\n",
    "- Careful with mask dimensions in self and cross attention\n",
    "- Key implementation detail for causal masking:\n",
    "- Create causal mask using upper triangular matrix:\n",
    "  ```python\n",
    "  mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "  mask = mask.masked_fill(mask == 1, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder layer of the Transformer\n",
    "    Sublayers: TransformerAttention with self-attention\n",
    "               Residual LayerNorm\n",
    "               TransformerAttention with cross-attention\n",
    "               Residual LayerNorm\n",
    "               FNN\n",
    "               Residual LayerNorm\n",
    "    Args:\n",
    "            d_model: 512 model hidden dimension\n",
    "            d_embed: 512 embedding dimension, same as d_model in transformer framework\n",
    "            d_ff: 2048 hidden dimension of the feed forward network\n",
    "            num_head: 8 Number of attention heads.\n",
    "            dropout:  0.1 dropout rate\n",
    "\n",
    "            bias: Whether to include bias in linear projections.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, num_head, dropout=0.1, bias=True):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        # attention sublayer\n",
    "        self.att = TransformerAttention(\n",
    "            d_model=d_model, num_head=num_head, dropout=dropout, bias=bias\n",
    "        )\n",
    "\n",
    "        # FFN sublayer\n",
    "        self.ffn = FFN(d_model=d_model, d_ff=d_ff)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # layer-normalization layer\n",
    "        self.LayerNorm_att1 = nn.LayerNorm(self.d_model)\n",
    "        self.LayerNorm_att2 = nn.LayerNorm(self.d_model)\n",
    "        self.LayerNorm_ffn = nn.LayerNorm(self.d_model)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_causal_mask(seq_len):\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float(\"-inf\"))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, embed_input, cross_input, padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        embed_input: Decoder input sequence [batch_size, seq_len, d_model]\n",
    "        cross_input: Encoder output sequence [batch_size, encoder_seq_len, d_model]\n",
    "        casual_attention_mask: Causal mask for self-attention [batch_size, seq_len, seq_len]\n",
    "        padding_mask: Padding mask for cross-attention [batch_size, seq_len, encoder_seq_len]\n",
    "        Returns:\n",
    "        Tensor: Decoded output [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = embed_input.size()\n",
    "\n",
    "        assert (\n",
    "            embed_input.size(-1) == self.d_model\n",
    "        ), f\"Input dimension {embed_input.size(-1)} doesn't match model dimension {self.d_model}\"\n",
    "        assert (\n",
    "            cross_input.size(-1) == self.d_model\n",
    "        ), \"Encoder output dimension doesn't match model dimension\"\n",
    "\n",
    "        # Generate and expand causal mask for self-attention\n",
    "        causal_mask = self.create_causal_mask(seq_len).to(\n",
    "            embed_input.device\n",
    "        )  # [seq_len, seq_len]\n",
    "        causal_mask = causal_mask.unsqueeze(0).unsqueeze(1)  # [1, 1, seq_len, seq_len]\n",
    "\n",
    "        ## First sublayer: self attion\n",
    "        # After embedding and positional encoding, input sequence feed into current attention sublayer\n",
    "        # Or, the output of the previous encoder/decoder feed into current attention sublayer\n",
    "        att_sublayer1 = self.att(\n",
    "            sequence=embed_input, key_value_states=None, att_mask=causal_mask\n",
    "        )  # [batch_size, num_head, sequence_length, d_model]\n",
    "        # apply dropout before layer normalization for each sublayer\n",
    "        att_sublayer1 = self.dropout(att_sublayer1)\n",
    "        # Residual layer normalization\n",
    "        att_normalized1 = self.LayerNorm_att1(\n",
    "            embed_input + att_sublayer1\n",
    "        )  # [batch_size, sequence_length, d_model]\n",
    "\n",
    "        ## Second sublayer: cross attention\n",
    "        # Query from the output of previous attention output, or training data\n",
    "        # Key, Value from output of Encoder of the same layer\n",
    "        att_sublayer2 = self.att(\n",
    "            sequence=att_normalized1,\n",
    "            key_value_states=cross_input,\n",
    "            att_mask=padding_mask,\n",
    "        )  # [batch_size, sequence_length, d_model]\n",
    "        # apply dropout before layer normalization for each sublayer\n",
    "        att_sublayer2 = self.dropout(att_sublayer2)\n",
    "        # Residual layer normalization\n",
    "        att_normalized2 = self.LayerNorm_att2(\n",
    "            att_normalized1 + att_sublayer2\n",
    "        )  # [batch_size, sequence_length, d_model]\n",
    "\n",
    "        # Third sublayer: FFN\n",
    "        ffn_sublayer = self.ffn(\n",
    "            att_normalized2\n",
    "        )  # [batch_size, sequence_length, d_model]\n",
    "        ffn_sublayer = self.dropout(ffn_sublayer)\n",
    "        ffn_normalized = self.LayerNorm_ffn(\n",
    "            att_normalized2 + ffn_sublayer\n",
    "        )  # [batch_size, sequence_length, d_model]\n",
    "\n",
    "        return ffn_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerDecoder(\n",
      "  (att): TransformerAttention(\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (output_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ffn): FFN(\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (LayerNorm_att1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (LayerNorm_att2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (LayerNorm_ffn): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = TransformerDecoder(d_model=512, d_ff=2048, num_head=8, dropout=0.1, bias=True)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder Stack\n",
    "Implements the full stack of encoder and decoder layers from Section 3.1.\n",
    "\n",
    "Key points:\n",
    "- Multiple encoder and decoder layers (typically 6)\n",
    "- Each encoder output feeds into all decoder layers\n",
    "- Maintains residual connections throughout the stack\n",
    "\n",
    "Implementation tips:\n",
    "- Use nn.ModuleList for layer stacks\n",
    "- Share encoder outputs across decoder layers\n",
    "- Maintain consistent masking throughout\n",
    "- Handle padding masks separately from causal masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder-Decoder stack of the Transformer\n",
    "    Sublayers:  Encoder x 6\n",
    "                Decoder x 6\n",
    "    Args:\n",
    "            d_model: 512 model hidden dimension\n",
    "            d_embed: 512 embedding dimension, same as d_model in transformer framework\n",
    "            d_ff: 2048 hidden dimension of the feed forward network\n",
    "            num_head: 8 Number of attention heads.\n",
    "            dropout:  0.1 dropout rate\n",
    "\n",
    "            bias: Whether to include bias in linear projections.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_layer, d_model, d_ff, num_head, dropout=0.1, bias=True):\n",
    "        super().__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.num_head = num_head\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "\n",
    "        # Encoder stack\n",
    "        self.encoder_stack = nn.ModuleList(\n",
    "            [\n",
    "                TransformerEncoder(\n",
    "                    d_model=self.d_model,\n",
    "                    d_ff=self.d_ff,\n",
    "                    num_head=self.num_head,\n",
    "                    dropout=self.dropout,\n",
    "                    bias=self.bias,\n",
    "                )\n",
    "                for _ in range(self.num_layer)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Decoder stack\n",
    "        self.decoder_stack = nn.ModuleList(\n",
    "            [\n",
    "                TransformerDecoder(\n",
    "                    d_model=self.d_model,\n",
    "                    d_ff=self.d_ff,\n",
    "                    num_head=self.num_head,\n",
    "                    dropout=self.dropout,\n",
    "                    bias=self.bias,\n",
    "                )\n",
    "                for _ in range(self.num_layer)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, embed_encoder_input, embed_decoder_input, padding_mask=None):\n",
    "        # Process through all encoder layers first\n",
    "        encoder_output = embed_encoder_input\n",
    "        for encoder in self.encoder_stack:\n",
    "            encoder_output = encoder(encoder_output, padding_mask)\n",
    "\n",
    "        # Use final encoder output for all decoder layers\n",
    "        decoder_output = embed_decoder_input\n",
    "        for decoder in self.decoder_stack:\n",
    "            decoder_output = decoder(decoder_output, encoder_output, padding_mask)\n",
    "\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Transformer\n",
    "Combines all components into complete architecture:\n",
    "- Input embeddings for source and target\n",
    "- Positional encoding\n",
    "- Encoder-decoder stack\n",
    "- Final linear and softmax layer\n",
    "\n",
    "Key points:\n",
    "- Handles different vocabulary sizes for source/target\n",
    "- Shifts decoder inputs for teacher forcing\n",
    "- Projects outputs to target vocabulary size\n",
    "- Applies log softmax for training stability\n",
    "\n",
    "Implementation tips:\n",
    "- Handle start tokens for decoder input\n",
    "- Maintain separate embeddings for source/target\n",
    "- Remember to scale embeddings\n",
    "- Consider sharing embedding weights with output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layer,\n",
    "        d_model,\n",
    "        d_embed,\n",
    "        d_ff,\n",
    "        num_head,\n",
    "        src_vocab_size,\n",
    "        tgt_vocab_size,\n",
    "        max_position_embeddings=512,\n",
    "        dropout=0.1,\n",
    "        bias=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "\n",
    "        # Source and target embeddings\n",
    "        self.src_embedding = EmbeddingWithProjection(\n",
    "            vocab_size=src_vocab_size,\n",
    "            d_embed=d_embed,\n",
    "            d_model=d_model,\n",
    "            max_position_embeddings=max_position_embeddings,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.tgt_embedding = EmbeddingWithProjection(\n",
    "            vocab_size=tgt_vocab_size,\n",
    "            d_embed=d_embed,\n",
    "            d_model=d_model,\n",
    "            max_position_embeddings=max_position_embeddings,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        # Encoder-Decoder stack\n",
    "        self.encoder_decoder = TransformerEncoderDecoder(\n",
    "            num_layer=num_layer,\n",
    "            d_model=d_model,\n",
    "            d_ff=d_ff,\n",
    "            num_head=num_head,\n",
    "            dropout=dropout,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "        # Output projection and softmax\n",
    "        self.output_projection = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def shift_target_right(self, tgt_tokens):\n",
    "        # Shift target tokens right by padding with zeros at the beginning\n",
    "        batch_size, seq_len = tgt_tokens.size()\n",
    "\n",
    "        # Create start token (zeros)\n",
    "        start_tokens = torch.zeros(\n",
    "            batch_size, 1, dtype=tgt_tokens.dtype, device=tgt_tokens.device\n",
    "        )\n",
    "\n",
    "        # Concatenate start token and remove last token\n",
    "        shifted_tokens = torch.cat([start_tokens, tgt_tokens[:, :-1]], dim=1)\n",
    "\n",
    "        return shifted_tokens\n",
    "\n",
    "    def forward(self, src_tokens, tgt_tokens, padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src_tokens: source sequence [batch_size, src_len]\n",
    "            tgt_tokens: target sequence [batch_size, tgt_len]\n",
    "            padding_mask: padding mask [batch_size, 1, 1, seq_len]\n",
    "        Returns:\n",
    "            output: [batch_size, tgt_len, tgt_vocab_size] log probabilities\n",
    "        \"\"\"\n",
    "        # Shift target tokens right for teacher forcing\n",
    "        shifted_tgt_tokens = self.shift_target_right(tgt_tokens)\n",
    "\n",
    "        # Embed source and target sequences\n",
    "        src_embedding = self.src_embedding(src_tokens)\n",
    "        tgt_embedding = self.tgt_embedding(shifted_tgt_tokens)\n",
    "\n",
    "        # Pass through encoder-decoder stack\n",
    "        decoder_output = self.encoder_decoder(\n",
    "            embed_encoder_input=src_embedding,\n",
    "            embed_decoder_input=tgt_embedding,\n",
    "            padding_mask=padding_mask,\n",
    "        )\n",
    "\n",
    "        # Project to vocabulary size and apply log softmax\n",
    "        logits = self.output_projection(decoder_output)\n",
    "        log_probs = self.softmax(logits)\n",
    "\n",
    "        return log_probs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
